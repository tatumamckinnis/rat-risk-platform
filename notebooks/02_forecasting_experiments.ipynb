{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Forecasting Model Experiments\n",
    "\n",
    "This notebook develops and evaluates time-series forecasting models for rat complaint prediction.\n",
    "\n",
    "**Models Evaluated:**\n",
    "- XGBoost (gradient boosting)\n",
    "- LSTM (deep learning)\n",
    "- Prophet (Facebook's time-series library)\n",
    "- Ensemble (weighted combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from src.data_preprocessing import load_master_dataset, get_train_val_test_split\n",
    "from src.feature_engineering import FeatureEngineer, create_sequences\n",
    "from src.forecasting_models import (\n",
    "    XGBoostForecaster, LSTMForecaster, ProphetForecaster,\n",
    "    EnsembleForecaster, evaluate_model, compare_models\n",
    ")\n",
    "from src import config\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load master dataset\n",
    "df = load_master_dataset()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split (chronological)\n",
    "train_df, val_df, test_df = get_train_val_test_split(df)\n",
    "\n",
    "print(f\"Train: {len(train_df)} ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
    "print(f\"Val: {len(val_df)} ({val_df['date'].min()} to {val_df['date'].max()})\")\n",
    "print(f\"Test: {len(test_df)} ({test_df['date'].min()} to {test_df['date'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "fe = FeatureEngineer()\n",
    "\n",
    "train_df_fe, y_train = fe.fit_transform(train_df)\n",
    "val_df_fe, y_val = fe.transform(val_df)\n",
    "test_df_fe, y_test = fe.transform(test_df)\n",
    "\n",
    "X_train = fe.get_feature_matrix(train_df_fe)\n",
    "X_val = fe.get_feature_matrix(val_df_fe)\n",
    "X_test = fe.get_feature_matrix(test_df_fe)\n",
    "\n",
    "print(f\"Features created: {len(fe.all_features)}\")\n",
    "print(f\"Training shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model\n",
    "\n",
    "Simple baseline: predict previous month's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: lag-1 prediction\n",
    "baseline_pred = test_df_fe['lag_1'].values * fe.scaler.scale_[0] + fe.scaler.mean_[0]\n",
    "baseline_metrics = evaluate_model(y_test, baseline_pred)\n",
    "\n",
    "print(\"Baseline (Previous Month) Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_model = XGBoostForecaster()\n",
    "xgb_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_metrics = evaluate_model(y_test, xgb_pred)\n",
    "\n",
    "print(\"XGBoost Metrics:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = xgb_model.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': fe.all_features,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance (Top 15)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "seq_len = 12\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_len)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, seq_len)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_len)\n",
    "\n",
    "print(f\"Sequence shape: {X_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "lstm_model = LSTMForecaster(input_size=X_train.shape[1], sequence_length=seq_len)\n",
    "lstm_model.fit(X_train_seq, y_train_seq, X_val_seq, y_val_seq, epochs=50)\n",
    "\n",
    "# Evaluate\n",
    "lstm_pred = lstm_model.predict(X_test_seq)\n",
    "lstm_metrics = evaluate_model(y_test_seq, lstm_pred)\n",
    "\n",
    "print(\"LSTM Metrics:\")\n",
    "for metric, value in lstm_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(lstm_model.training_history['train_loss'], label='Train')\n",
    "ax[0].plot(lstm_model.training_history['val_loss'], label='Validation')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('LSTM Training Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(y_test_seq, lstm_pred, alpha=0.5)\n",
    "ax[1].plot([0, max(y_test_seq)], [0, max(y_test_seq)], 'r--')\n",
    "ax[1].set_xlabel('Actual')\n",
    "ax[1].set_ylabel('Predicted')\n",
    "ax[1].set_title('LSTM: Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet (one model per location)\n",
    "prophet_model = ProphetForecaster()\n",
    "prophet_model.fit(train_df, date_col='date', target_col='complaint_count', group_col='zip_code')\n",
    "\n",
    "# Evaluate\n",
    "prophet_pred = prophet_model.predict(test_df)\n",
    "prophet_metrics = evaluate_model(y_test[:len(prophet_pred)], prophet_pred)\n",
    "\n",
    "print(\"Prophet Metrics:\")\n",
    "for metric, value in prophet_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'XGBoost', 'LSTM', 'Prophet'],\n",
    "    'MAE': [baseline_metrics['mae'], xgb_metrics['mae'], lstm_metrics['mae'], prophet_metrics['mae']],\n",
    "    'RMSE': [baseline_metrics['rmse'], xgb_metrics['rmse'], lstm_metrics['rmse'], prophet_metrics['rmse']],\n",
    "    'R²': [baseline_metrics['r2'], xgb_metrics['r2'], lstm_metrics['r2'], prophet_metrics['r2']],\n",
    "    'MAPE': [baseline_metrics['mape'], xgb_metrics['mape'], lstm_metrics['mape'], prophet_metrics['mape']],\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "metrics_to_plot = ['MAE', 'RMSE', 'R²']\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    ax.bar(results['Model'], results[metric])\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(metric)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted ensemble\n",
    "ensemble = EnsembleForecaster(\n",
    "    models={'xgboost': xgb_model, 'lstm': lstm_model, 'prophet': prophet_model},\n",
    "    weights={'xgboost': 0.45, 'lstm': 0.35, 'prophet': 0.20}\n",
    ")\n",
    "\n",
    "# For ensemble prediction, we need aligned predictions\n",
    "# Using XGBoost predictions as primary\n",
    "ensemble_pred = ensemble.predict(X_test, test_df)\n",
    "ensemble_metrics = evaluate_model(y_test, ensemble_pred)\n",
    "\n",
    "print(\"Ensemble Metrics:\")\n",
    "for metric, value in ensemble_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "errors = y_test - xgb_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Error distribution\n",
    "axes[0].hist(errors, bins=30, edgecolor='black')\n",
    "axes[0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Prediction Error')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Error Distribution')\n",
    "\n",
    "# Residual plot\n",
    "axes[1].scatter(xgb_pred, errors, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Value')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(errors, dist='norm', plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('../models/forecasting')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xgb_model.save(str(save_dir / 'xgboost.joblib'))\n",
    "lstm_model.save(str(save_dir / 'lstm.pt'))\n",
    "prophet_model.save(str(save_dir / 'prophet.joblib'))\n",
    "fe.save(str(save_dir / 'feature_engineer.joblib'))\n",
    "\n",
    "print(\"Models saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
